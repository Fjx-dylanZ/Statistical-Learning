---
title: 'Problem Set #4'
author: "Kevin McAlister"
date: "February 9th, 2022"
output:
  pdf_document:
    toc: no
    toc_depth: '2'
  prettydoc::html_pretty:
    df_print: kable
    highlight: github
    theme: leonids
    toc: no
    toc_depth: 2
    toc_float:
      collapsed: no
  html_document:
    df_print: paged
    toc: no
    toc_depth: '2'
urlcolor: blue
---

```{r, include=FALSE}
library(ggplot2)
library(data.table)
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning = FALSE, fig.width = 16/2, fig.height = 9/2, tidy.opts=list(width.cutoff=60), tidy=TRUE)
```

This is the fourth problem set for QTM 385 - Intro to Statistical Learning. This homework will cover a derivation related to splines and two applied exercises related to smoothing splines, GAMs, and simple regression trees.

Please use the intro to RMarkdown posted in the Intro module and my .Rmd file as a guide for writing up your answers. You can use any language you want, but I think that a number of the computational problems are easier in R. Please post any questions about the content of this problem set or RMarkdown questions to the corresponding discussion board.

Your final deliverable should be two files - a .Rmd/.ipynb file and either a rendered HTML file or a PDF. Students can complete this assignment in groups of up to 3. Please identify your collaborators at the top of your document. All students should turn in a copy of the solutions, but your solutions can be identical to those of your collaborators.

This assignment is due by February 18th, 2022 at 11:59 PM EST.

------------------------------------------------------------------------

## Problem 1 (20 pts)

Regression splines are a broadly applicable method for regression analysis because they can be represented and estimated as an augmented OLS problem.

A generic cubic regression spline with $K$ knots can be represented as a linear model with $K + 4$ basis expansion terms:

$$h_1(x) = 1 \text{   ;   } h_{\text{2 to 4}}(x) = \{x,x^2,x^3\}$$

$$h_{\text{5 to K + 4}} = \{(x - \xi_1)_+^3 ,  (x - \xi_2)_+^3,...,(x - \xi_K)_+^3\}$$

$$y_i = \alpha + \sum \limits_{k = 2}^{K + 4} \beta_k h_k(x_i) + \epsilon_i$$ Cubic regression splines fit a function to the data that is continuous with respect to $x$ and continuous in its first two derivatives.

For an arbitrary collection of $K \le N$ knots, prove that the cubic regression spline provides a function that is continuous in the first and second derivative at the knots.

Notes:

1.  You can assume that the function is class $C^2$ continuous away from the knots. This is true and provable, but you can just take that for granted without further explanation.

2.  With an appropriate argument about the relationship between continuity in the 1st and 2nd derivatives, you don't need to show continuity on the first derivatives.

## Problem 2 (80 pts)

The data sets `college_train.csv` (600 observations) and `college_test.csv` (177 observations) include information about different colleges in the U.S. We're going to use this data set to try to build a model that predicts the logarithm of out of state tuition for a college using a variety of predictors related to college quality. Note that this data was collected back in 1995 - a magical time in U.S. history where "Run Around" by Blues Traveler was playing on the radio, Bill Clinton had a plan to actually balance the U.S. budget, and young Dr. McAlister learned to tie his shoes in Ms. Lamb's first grade class. It is also notable that college used to be affordable! Don't be too downtrodden when you look at this data set and see Emory's out of state tuition back then...

As always, the test set is intended to be used only for quantifying expected prediction error after choosing some trained models. A description of the variables in the data set can be found [here](https://www.rdocumentation.org/packages/ISLR/versions/1.4/topics/College). I've added an additional predictor called `AcceptRate` which is the acceptance rate of the school in 1995.

Note: I would recommend just recoding `Outstate = log(Outstate)` right at the beginning.

### Part 1 (20 pts.)

Let's start by looking at a single predictor - the student/faculty ratio `S.F.Ratio`. Plot log out of state tuition against the student/faculty ratio. Does this look linear?

Using a measure of expected prediction error appropriate for a standard linear model, find the order of global polynomial that minimizes EPE. Be sure to note your choice and why you made it. Using this value, train your model on the full training set and plot the prediction curve implied by the polynomial model on your graph.

Next, estimate a cubic regression spline, a cubic natural spline, and a smoothing spline using the entire training data. For the regression spline and natural spline, you need to choose the number of knots or degrees of freedom. I would recommend setting these to 5 to start and playing with it until you get something that looks right. For the smoothing spline, you should choose the final form using a built-in cross-validation method (most likely GCV). Add the prediction curve to your plot. How do the drawn curves differ between methods?

Finally, use your polynomial model and splines to create predictions for the test set and quantify the mean squared error for the test set. Which model performs best? Worst? Provide some rationale for this outcome.

```{r}
library(reticulate)
use_virtualenv("../../venv")
```

```{python}
import pandas as pd
import numpy as np
import re
import matplotlib.pyplot as plt
import statsmodels.api as sm
# Import Data
train_df = pd.read_csv("college_train.csv")
test_df = pd.read_csv("college_test.csv")

fig, ax = plt.subplots(figsize=(10,10))
train_df.plot(x='S.F.Ratio', y='Outstate', kind='scatter', ax=ax)
plt.show()
```

#### Find the best-performing OLS with polynomial basis expansion

```{python}
from itertools import combinations
def get_basis(degree: int):
    possible_comb = []
    for j in range(1, degree+1):
        for i in list(combinations(list(range(1,degree+1)), j)):
            if degree in i: possible_comb.append(i)
    return possible_comb
get_basis(4)
```

```{python}
import statsmodels.api as sm
def ols_ploy_fit(train_x, train_y, **kwargs):
    nobvs = len(train_x)
    max_param = nobvs - 2
    #if len(train_x.ravel()) > nobvs: raise Exception("err")
    ols_LOOCV = pd.DataFrame(columns=['Degree', 'Basis', 'LOOCV', 'y_pred'])
    for degree in range(1, 10):
        for basis in get_basis(degree):
            train_x_ols = pd.DataFrame()
            for paramDegree in basis:
                train_x_ols[f'S.F.Ratio^{paramDegree}'] = train_x ** paramDegree
            train_x_ols.insert(0, 'intercept', np.ones(nobvs))
            ols_reg = sm.OLS(endog=train_y, exog=train_x_ols)
            ols_reg_result = ols_reg.fit()
            train_y_predict = ols_reg_result.predict(train_x_ols)
            hat_diag = ols_reg_result.get_influence().summary_frame()['hat_diag']
            loocv_estimate = 1/nobvs * np.sum(
                ((train_y - train_y_predict)/(np.ones(nobvs) - hat_diag)) ** 2
            )
            ols_LOOCV = ols_LOOCV.append({'Degree': degree,
             'Basis': str(basis), 'LOOCV': loocv_estimate, 'y_pred': train_y_predict}, ignore_index=True)
            #print(f"Degree: {basis}, LOOCV: {loocv_estimate}")
    return ols_LOOCV
ols_LOOCV = ols_ploy_fit(train_df['S.F.Ratio'].to_frame(), train_df['Outstate'])
ols_LOOCV = ols_LOOCV.sort_values('LOOCV', ascending=True)
```

```{python}
ols_LOOCV.iloc[:10,0:-1]
```

We can see that degree 4 polynomial with basis $x^2+x^4$ minimizes the expected prediction error.

```{python}
# USE intercept + x^2 + x^4
fig, ax = plt.subplots(figsize=(12,10))
ax.scatter(x=train_df['S.F.Ratio'], y = train_df['Outstate'], c = 'b', alpha=0.4, label='True')
ax.scatter(x=train_df['S.F.Ratio'], y = ols_LOOCV.iloc[0, -1], c='orange', label='basis: (2,4)')
ax.legend()
ax.set_xlabel('Student/faculty ratio')
ax.set_ylabel('Out-of-state tuition')
plt.show()
```

### Part 2 (15 pts.)

Now, let's consider the multivariate case with all of the predictors. Let's improve on the standard linear model by using LASSO to do some variable selection and shrinkage. Fit the LASSO model to the training data and use K-fold cross validation to select a value of $\lambda$. Be sure to explain why you made the choice that you did. How many variables are used in the "optimal" model? Be sure to record the optimal $\lambda$ for later use.

```{python}
train_x = train_df.drop(['Outstate', 'CollegeNames'], axis=1)\
    .assign(Private = train_df['Private']\
        .map({'Yes': 1, 'No': 0}))
train_y = train_df['Outstate']
```
```{r}
library(tidyverse)

lasso_cv = glmnet::cv.glmnet(x=data.matrix(py$train_x), 
                  y=data.matrix(py$train_y), type.measure = 'mse', nfolds = 30,
                  family = 'gaussian',
                  alpha = 1,
                  intercept = TRUE
                  )
plot(lasso_cv)
lasso_cv$lambda.min+sd(lasso_cv$lambda)
lasso_cv$lambda.1se
```
### Part 3 (15 pts.)

Now, let's see if we can improve on the standard LASSO regression model using a GAM. There are three approaches you can take here:

1.  Use all of the predictors.
2.  Only use the predictors selected by LASSO under your optimal model.
3.  Try to fit the GAM with an even smaller model using a subset of predictors from a higher sparsity point on the LASSO path.

Any of these approaches are fine for this problem. However, your model should include `S.F.Ratio` and `Private` (I'm pretty sure that both of these will pop up early in the LASSO path). Subset selection for GAMs is something that people are working on! There are some ways to build-in LASSO style penalties, but it's difficult to determine what a zero even is - since these are functions instead of linear combinations, what would we even shrink?

Try different combinations of linear terms, spline terms, and think-plate/tensor spline terms to try to minimize the GCV associated with your GAM. Most implementations will return this as part of the model object.

Create a plot that shows the function for each predictor. Do the marginal relationships make sense? For any 2-predictor spline terms, do they capture anything that wouldn't be captured by a linear model? Look at your function for `S.F.Ratio`. Does it look like the smoothing spline you uncovered in part 1? What does this say about the plausibility of the additivity assumption for this specific variable?

Be sure to note your "optimal" model and why you chose that one. Your search doesn't need to be exhaustive (that's impossible), just a reasonable attempt to get a good predictive model!

Compare the GCV for your chosen GAM to the K-fold CV estimate for LASSO. Does capturing non-linearities within the data improve prediction accuracy?

### Part 4 (15 pts.)

Finally, let's use the training data to build a single regression tree. Using a CART implementation, grow a regression tree over all predictors. If you're interested, try changing some of the stopping criteria for the regression tree growth procedure and see how deep the tree goes.

Create a graphical representation of this tree. What variables are selected by the CART procedure? Does this line up with the LASSO choices?

Using cross-validation, find an "optimal" tree size with respect to the cost-complexity criterion. Use your choice to create a pruned tree and plot it. How does the pruned tree differ from the full tree?

Be sure to save the model objects for the full and pruned trees. These will be used to create predictions for the test set.

Note that we don't yet have a measure of expected prediction accuracy for trees! For now, don't worry about that.

### Part 5 (15 pts)

Finally, let's compare the three models to see which one performs best on the out of sample data. Create predictions using your LASSO model, GAM, full tree, and pruned tree for the out of sample test data. Use these predictions to compute the out of sample MSE for each method. Which performs best? Worst?

In a few sentences, discuss when you think LASSO will work better than GAMs and vice versa. Given our toolset from this week, trees need some work. And we'll do that next week!
